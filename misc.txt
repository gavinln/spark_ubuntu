To build the Spark image
1. Go to the Spark Docker container directory
cd /vagrant/docker/spark

2. Build the Spark image
sudo docker build --rm -t sequenceiq/spark:1.1.1 .

3. Run the Spark image
sudo docker run -i -t sequenceiq/spark:1.1.1 /etc/bootstrap.sh -bash

4. Change to the Spark folder
cd /usr/local/spark

4. Run pyspark
./bin/pyspark

5. Create a RDD (Resilient Distributed Dataset)
textFile = sc.textFile('file:///usr/local/spark-1.1.1-bin-hadoop2.4/README.md')

6. Apply the count action to count the number of lines
textFile.count()

7. Apply a transformation that only returns lines containing "Spark"
textFile = sc.textFile('file:///usr/local/spark-1.1.1-bin-hadoop2.4/README.md')

8. Chain the transformation and action
textFile.filter(lambda line: "Spark" in line).count() # How many lines contain "Spark"?

