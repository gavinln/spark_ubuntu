To build the Spark image
1. Go to the Spark Docker container directory
cd /vagrant/docker/spark

2. Build the Spark image
sudo docker build --rm -t sequenceiq/spark:1.1.1 .

3. Run the Spark image
sudo docker run -i -t sequenceiq/spark:1.1.1 /etc/bootstrap.sh -bash

4. Change to the Spark folder
cd /usr/local/spark

4. Run pyspark
./bin/pyspark

5. Create a RDD (Resilient Distributed Dataset)
textFile = sc.textFile('file:///usr/local/spark-1.1.1-bin-hadoop2.4/README.md')

6. Apply the count action to count the number of lines
textFile.count()

7. Apply a transformation that only returns lines containing "Spark"
textFile.filter(lambda line: "Spark" in line)

8. Chain the transformation and action
textFile.filter(lambda line: "Spark" in line).count() # How many lines contain "Spark"?

9. Get lengths of each line
lineLengths = textFile.map(len)

10. Get count of each word
word_counts = textFile.flatMap(lambda x: x.split(' ')).map(lambda s: (s, 1))
counts = word_counts.reduceByKey(lambda a, b: a + b)

11. Display top 10 frequently occuring words
counts.sortBy(lambda x: x[1], ascending=False).take(10)
